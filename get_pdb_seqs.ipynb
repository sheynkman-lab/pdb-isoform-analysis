{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import csv\n",
    "up_one =  (os.path.abspath(os.path.join(os.getcwd(), os.pardir)))\n",
    "sys.path.append(up_one)\n",
    "up_two =  (os.path.abspath(os.path.join(os.getcwd(), os.pardir, os.pardir)))\n",
    "sys.path.append(up_two)\n",
    "from minorlab import metapdb\n",
    "import subprocess\n",
    "from collections import defaultdict\n",
    "import datetime\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn={}  # A dict of connections\n",
    "cursor={}    # A dict of cursors\n",
    "engine={}\n",
    "def connect_db(db):\n",
    "    print (f\"     Connecting to {db['name']}...\")\n",
    "    try:\n",
    "        #print (f\"\"\"    con_string is { db['con_string'] }\"\"\")\n",
    "        exec (f\"\"\"conn[ \"{db[\"name\"]}\" ] = psycopg2.connect(\"{db[\"con_string\"]}\")\"\"\" )\n",
    "        exec (f\"\"\"cursor[ \"{db['name']}\" ] = conn[ \"{db['name']}\" ].cursor()\"\"\") # Pandas read_sql_query does not need a cursor\n",
    "        exec (f\"\"\"engine[\"{db['name']}\"] = create_engine( \"{db['engine_string']}\") \"\"\")\n",
    "        print(f\"  Connected to { db['name'] }.\")\n",
    "    except:\n",
    "        print ('Check VPN. Not Connected.   <<<<===========#################')\n",
    "connect_db(metapdb)\n",
    "#connect_db(manuscripts)\n",
    "print ('Connections done.')\n",
    "# Run this cell to use single database functions\n",
    "con = conn['metapdb']\n",
    "cur = cursor['metapdb']\n",
    "eng = engine['metapdb']\n",
    "\n",
    "def df_to_empty_table(df, table,db='metapdb'):\n",
    "    #create table using SLQalchemy \n",
    "    parts = table.split('.')\n",
    "    if len(parts) ==1:\n",
    "        schema='public'\n",
    "        table_name=table\n",
    "    else:\n",
    "        schema, table_name = parts\n",
    "    empty_df=  df.loc[df.iloc[:,0]=='Nonsense']\n",
    "    try:\n",
    "        empty_df.to_sql( schema=schema,name=table_name, con=engine[db], if_exists = 'replace', index=False)\n",
    "        print (f'  Empty table {table} created for DataFrame with columns {empty_df.columns.to_list()}')\n",
    "    except Exception as error:\n",
    "        print (f'There was a problem with the SQLalchemy part. No table created.\\n{error}')\n",
    "        return 0\n",
    "    return 1\n",
    "    \n",
    "def df_to_table(df, table,db='metapdb'):\n",
    "    ''' Loads a DataFrame into a table. \n",
    "        If table is in the form schema.table, the schema will be used, otherwise schema=\"public\".\n",
    "        3rd ARG is databse; metapdb is default.'''\n",
    "    if df_to_empty_table(df,table,db) == 0:\n",
    "        return 0\n",
    "    # save dataframe to an in memory buffer\n",
    "    buffer = StringIO()\n",
    "    \n",
    "    df.to_csv(buffer, index=False, header=False,sep=';',quoting=csv.QUOTE_NONE,escapechar='\\\\')\n",
    "    buffer.seek(0)\n",
    "    \n",
    "    # Try to copy from the buffer to the table\n",
    "    try:\n",
    "        print (f\"\"\"  Starting copy_from(buffer)\"\"\" )\n",
    "        cursor[db].copy_from(buffer, table, sep=\";\",)\n",
    "        conn[db].commit()\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(\"Error: %s\" % error)\n",
    "        conn[db].rollback()\n",
    "        return 1\n",
    "    print(f\"df_to_table done for {table}.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slight reformat of gencode information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('Data/gencode.v43.pc_translations.fa', 'r') as f_in, open ('Data/gencode_v43_translations_mod.fa', 'w') as f_out:\n",
    "    for line in f_in.readlines():\n",
    "        if line[0] == \">\":\n",
    "            parts = line.split('|')\n",
    "            f_out.write(f\">{parts[5]}|{parts[1]}\\n\")\n",
    "        else:\n",
    "            f_out.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get list of human pdbs directly from PDB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_pdb= pd.read_csv ('Data/PDBe_human_entities.csv')\n",
    "human_pdb_red = human_pdb.loc[:,['pdb_id','entity_id']]\n",
    "df_to_table(human_pdb_red, 'temp_dcoop.human_pdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"select ep.pdbid, ep.pdbx_strand_id as chains\n",
    "            , ep.pdbx_seq_one_letter_code_can  as seq\n",
    "        from temp_dcoop.human_pdb hp\n",
    "        join pdbj.entity_poly ep on ep.pdbid = hp.pdb_id\n",
    "            and hp.entity_id = cast(ep.entity_id as INT)\n",
    "        where ep.type='polypeptide(L)'\n",
    "        order by pdbid, pdbx_strand_id\"\"\"\n",
    "df = pd.read_sql (sql,eng)\n",
    "df\n",
    "len (df['pdbid'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write out as pdb_human_seqs.fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('Data/pdb_human_seqs-230227.fa', 'w') as f:\n",
    "    for i,parts in df.iterrows():\n",
    "        chains = chains.replace(',','_')\n",
    "        f.write(f\">{parts['pdbid']}_{parts['chains']}\\n{parts['seq']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eliminate seqs < 30 AA and group by 100% identity\n",
    "\n",
    "Group by reduces the list from ~97K to ~39K  \n",
    "Removing chains less than 30AA removes 3167 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df.copy()\n",
    "df2['id']=df2['pdbid']+'_'+ df2['chains']\n",
    "df2=df2.loc[:,['id','seq']]\n",
    "df2 = df2.loc[ df2['seq'].str.len() >= 30 ]\n",
    "print (f'df({len(df)}) - df2({len(df2)}) = {len(df) -  len(df2)} removed by 30 AA cutoff') \n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = df2.groupby('seq')['id'].apply(list)\n",
    "df2_g = groups.reset_index(name='chains')\n",
    "print (f'df2({len(df2)}) - df2_g({len(df2_g)}) = {len(df2) -  len(df2_g)} less seqs after clustering at 100%') \n",
    "df2_g.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get representative chain as for name and write out .fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_g['rep'] = [i[0] for i in df2_g['chains']]\n",
    "with open ('Data/pdb_unique_human_seqs_230301.fa', 'w') as f:\n",
    "    for i,parts in df2_g.iterrows():\n",
    "        f.write(f\">{parts['rep']}\\n{parts['seq']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blast part\n",
    "blastp -db ./a_gencode_fasta_same_prot_clustered.v35.fa -query ./uniprot_reviewed_canonical_and_isoform.fasta  \n",
    "      -outfmt \"6 qseqid qlen qstart qend sseqid slen sstart send evalue bitscore length nident gaps\"   \n",
    "      -out run_UN_to_GC_blast_result.tsv -num_threads 24 >> run_UN_to_GC_blast_result_stdout.txt 2>&1\n",
    "makeblastdb -in <fasta file> -out <database name> -dbtype <type> -title <title> -parse_seqids\n",
    "    \n",
    "__[TEXT](https://www.ncbi.nlm.nih.gov/books/NBK279684/table/appendices.T.options_common_to_all_blast/)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blast_path = \"/home/daroc/ncbi-blast-2.13.0+/bin\"\n",
    "blastp = os.path.join (blast_path, 'blastp')\n",
    "makedb = os.path.join (blast_path, 'makeblastdb')\n",
    "s='gencode_v43_translations_mod.fa'\n",
    "#q='one_pdb.fa'\n",
    "#q='ten_percent_pdb.fa'\n",
    "#q='some_pdbs.fa'\n",
    "q='pdb_unique_human_seqs_230301.fa'\n",
    "blast_options=\"-evalue 1e-20 -max_hsps 2 -num_threads 8\"\n",
    "\n",
    "columns =  ['qseqid','qlen', 'qstart', 'qend', 'sseqid', 'slen', 'sstart', 'send', 'evalue', 'bitscore', 'length', 'nident', 'gaps','gapopen', 'mismatch', 'pident']\n",
    "column_str = \" \".join(columns)\n",
    "s_root = s.replace('.fa','')\n",
    "outfmt = f\"6 {column_str}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "makeblastdb ('gencode_v43_translations_mod.fa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Project specific functions\n",
    "# Make blast dbs\n",
    "def makeblastdb (fasta):\n",
    "    #  Note : This was problematic on OneDrive \n",
    "    root = fasta.replace('.fa','')\n",
    "    cmd = f\"\"\"{makedb} -in {fasta} -out {root} -dbtype prot -title \"Blastdb for {fasta}\" -parse_seqids \"\"\"\n",
    "    print (cmd)\n",
    "    db=subprocess.getoutput(cmd)\n",
    "    if db[0:5]==\"USAGE\":\n",
    "        print (f\"Something went wrong with creation of the database\\n{db}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make Databases in some cases\n",
    "\n",
    "NOTE:  This will not work from OneDrive directory becuase of the spaces in the name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Should not need to edit below here \n",
    "###\n",
    "makeblastdb(q)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run blast with tabular output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DF output\n",
    "start_time = datetime.datetime.now()\n",
    "print (f'Starting at {start_time}')\n",
    "blast_cmd = f'{blastp} -query \"Data/{q}\" -db \"Data/Blast_databases/{s_root}\" {blast_options} -outfmt \"{outfmt}\"'\n",
    "print (f\"{blast_cmd}\\n\")\n",
    "blast_results=subprocess.getoutput(blast_cmd)\n",
    "results_list = [line.split('\\t') for line in blast_results.splitlines()]\n",
    "blast_df = pd.DataFrame(results_list, columns = columns)\n",
    "end_time =  datetime.datetime.now()\n",
    "print ( f'It is now {end_time}. Blast took { end_time - start_time}')\n",
    "try:\n",
    "    blast_df.to_excel('Data/pdb_vs_gencode.xlsx')\n",
    "except:\n",
    "    print (\"\\n\\nCouldn't write Excel file!!!\")\n",
    "blast_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out full blast results  \n",
    "Note: it is too large to fit in one Excel spreadsheet. Write out as tsv file and 2 partial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blast_df.to_csv('Data/blast_fullPDB_vs_gencode.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blast_df.iloc[0:1000000].to_excel('Data/pdb_vs_gencode-part1.xlsx')\n",
    "blast_df.iloc[1000001:].to_excel('Data/pdb_vs_gencode-part2.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run blast and see full results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For full results including alignments\n",
    "blast_cmd = f'{blastp} -query \"Data/{q}\" -subject \"Data/{s}\"  {blast_options}'\n",
    "print (f\"{blast_cmd}\\n\")\n",
    "blast_results=subprocess.getoutput(blast_cmd)\n",
    "print (blast_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The function blast_one will extract one PDB seq and run blast on it. \n",
    "USE: blast_one(pdb, [align=False] )\n",
    "The default will give tabular result. align=True will give alignments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blast_df = blast_one('3iej_A,B')\n",
    "blast_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blast_df.to_excel('Data/3iej.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to reduce the data to top matches of each PDB\n",
    "Function is called top_five, the the second option is the numner of hits to include"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_five(df, max=5 ):\n",
    "    keep= []\n",
    "    last = \"\"\n",
    "    for i,row in df.iterrows():\n",
    "        if row['qseqid'] != last:\n",
    "            count=1\n",
    "            last = row['qseqid']\n",
    "            keep.append(True)\n",
    "        else:\n",
    "            count += 1\n",
    "            if count<=max:\n",
    "                keep.append(True)\n",
    "            else:\n",
    "                keep.append(False)\n",
    "    return (df.loc[keep])\n",
    "#top_five_df = top_five(copy_df)\n",
    "#top_five_df.to_excel('top5_pdb_vs_gencode.xlsx')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_2_df = top_five(blast_df, 2)\n",
    "top_2_df.to_csv('Data/top2_pdb_vs_gencode.tsv', sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_five_df.to_excel(\"Data/top_five.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify genes with more than one isoform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_check = top_2_df.copy()\n",
    "# Build a dictionary where the values are a set of isoforms for each gene\n",
    "genes=defaultdict(set)\n",
    "for id in df_to_check['sseqid']:\n",
    "    try:\n",
    "        id = id.split('|')[0]\n",
    "        parts = id.split('-')\n",
    "        gene='-'.join(parts[0:-1])\n",
    "        iso=parts[-1]\n",
    "        #print (id, gene, iso)\n",
    "        genes[gene].add(iso)\n",
    "    except:\n",
    "        pass\n",
    "# Create a list (check) of genes that have more than 1 isoform represented.\n",
    "check = []\n",
    "for g in genes:\n",
    "    if len(genes[g]) > 1:\n",
    "        #print (g,genes[g])\n",
    "        check.append(g)\n",
    "print (f'check is {len(check)} long')\n",
    "\n",
    "# Create list of df indicies of all genes that have more than one df \n",
    "found=[]\n",
    "for i,row in df_to_check.iterrows():\n",
    "    #print (row['sseqid'].split('-')[0])\n",
    "    try:\n",
    "        if row['sseqid'].split('-')[0] in check:\n",
    "            found.append(i)\n",
    "    except:\n",
    "        pass\n",
    "#print (f'found is {found}')\n",
    "found_df = df_to_check.loc[found]\n",
    "found_df.sort_values(by=['sseqid'], inplace=True)\n",
    "found_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_df.to_excel('Data/found_isos_from_2.xlsx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to extract sequences of Isoforms and related sequences\n",
    "USE: get_gene_seqs (gene, df=found_df)  \n",
    "By default, will extract data from found_df, but can be specified "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'found_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m     sequence \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sequence\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_gene_seqs\u001b[39m (gene, df\u001b[38;5;241m=\u001b[39m\u001b[43mfound_df\u001b[49m):\n\u001b[1;32m     19\u001b[0m     matches \u001b[38;5;241m=\u001b[39m found_df\u001b[38;5;241m.\u001b[39mloc[ found_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msseqid\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mcontains(gene) ]\n\u001b[1;32m     20\u001b[0m     isos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'found_df' is not defined"
     ]
    }
   ],
   "source": [
    "pdb_file='Data/pdb_unique_human_seqs_230301.fa'\n",
    "gene_file='Data/gencode_v43_translations_mod.fa'  \n",
    "def extract_sequence(filename, sequence_id):\n",
    "    sequence = \"\"\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\">\") and sequence_id in line:\n",
    "                sequence += line\n",
    "                # Found the sequence ID we want\n",
    "                while True:\n",
    "                    line = f.readline().strip()\n",
    "                    if line.startswith(\">\") or not line:\n",
    "                        # Reached the end of the sequence\n",
    "                        break\n",
    "                    sequence += line\n",
    "    sequence += \"\\n\"\n",
    "    return sequence\n",
    "def get_gene_seqs (gene, df=found_df):\n",
    "    matches = found_df.loc[ found_df['sseqid'].str.contains(gene) ]\n",
    "    isos = set()\n",
    "    pdbs = set()\n",
    "    for i,row in matches.iterrows():\n",
    "        isos.add(row['sseqid'])\n",
    "        pdbs.add(row['qseqid'])\n",
    "    seqs=\"\"\n",
    "    for pdb in pdbs:\n",
    "        seqs+= extract_sequence (pdb_file, pdb)\n",
    "    for iso in isos:\n",
    "        seqs+= extract_sequence (gene_file, iso)\n",
    "    return (seqs)\n",
    "\n",
    "s=get_gene_seqs('SF3B1')\n",
    "print (s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a directory with sequence files for all the matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each gene in found_df, create a FASTA file with PDB and isoform sequences. \n",
    "# Also creates a list (done) of the gene names\n",
    "done = []\n",
    "for id in found_df['sseqid']:\n",
    "    try:\n",
    "        id = id.split('|')[0]\n",
    "        parts = id.split('-')\n",
    "        gene='-'.join(parts[0:-1])\n",
    "        if gene not in done:\n",
    "            done.append(gene)\n",
    "            s = get_gene_seqs(gene)\n",
    "            with open (f'Data/Seq_groups/{gene}.fa', 'w') as out:\n",
    "                out.write(s)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf= pd.DataFrame(done)\n",
    "tdf.to_excel('gene_notes.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last = \"ABCB8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fresh=0\n",
    "for g in tdf[0]:\n",
    "    print (g)\n",
    "    if fresh==0 and g!=last:\n",
    "        continue\n",
    "    fresh=1\n",
    "    last = g\n",
    "    cmd = f'clustalo -i Data/Seq_groups/{g}.fa  --outfmt=clu -o Data/Seq_groups/{g}.msa'\n",
    "    print (cmd)\n",
    "    r=subprocess.getoutput(cmd)\n",
    "    print (r)\n",
    "    with open (f\"Data/Seq_groups/{g}.msa\") as f:\n",
    "        rr=f.read()\n",
    "        print (rr)\n",
    "        \n",
    "    read = input (\"Ready to move on? f to see sequence file \")\n",
    "    if read.lower() == 'f':\n",
    "        with open (f'Data/Seq_groups/{g}.fa') as seqs:\n",
    "            print (seqs.read())\n",
    "        read = input (\"Ready to move on? f to see sequence file \")"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
